# -*- coding: utf-8 -*-
"""predictiveModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMk_HO-sm_ELlGVCNO_0d94cq0HJ7IvJ
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense,LSTM,Dropout,Conv1D,GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc
from scipy import interp
import matplotlib.pyplot as plt
import time
import keras
from keras.utils.vis_utils import plot_model

sequences = pd.read_csv('sequences.csv', delimiter = ',')
print("Shape:",sequences.shape)

X = sequences["Sequence"]
Y = sequences["Label"].astype(dtype='int')

vocab_size = 250

X_encoded = [one_hot(seq, vocab_size,filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~',lower=True, split=' ') for seq in X]
print(X_encoded[1])

max_length = 25

X_encoded = pad_sequences(X_encoded, maxlen=max_length, padding='pre')
print(X_encoded[1])

nfold = 3
kfold = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42) #Used for cross-validation splitting
nEpoch = 10
nBatch = 128
embedding_vecor_length = 32

print("Model#1: CNN")
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)


start = time.time()
hists_CNN = []
for train, test in kfold.split(X_encoded, Y):
  model = Sequential()
  model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_length))
  model.add(Conv1D(16, 3, activation='relu'))
  model.add(GlobalMaxPooling1D())
  model.add(Dense(10, activation='relu'))
  model.add(Dense(1,activation='sigmoid'))

  adam = keras.optimizers.Adam(lr=0.001, beta_1=0.8, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

  model.compile(optimizer=adam,
                loss='binary_crossentropy',
                metrics=['accuracy'])
  
  #model.summary()
  
  hist = model.fit(X_encoded[train], Y[train],batch_size=nBatch ,epochs=nEpoch, verbose=1,validation_split=0.30)
  pred = model.predict(X_encoded[test]).ravel()
  fpr, tpr, thresholds = roc_curve(Y[test], pred)
  tprs.append(interp(mean_fpr, fpr, tpr))
  tprs[-1][0] = 0.0
  roc_auc = auc(fpr, tpr)
  print(roc_auc)
  aucs.append(roc_auc)
  hists_CNN.append(hist)
  

print("Avg AUC:", np.mean(aucs))

end = time.time()
print("Training Time:", end - start, "seconds.")

# Line line from origin represnting random predictions
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black',alpha=0.3)
#Adding to ROC plot
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
plt.plot(mean_fpr, mean_tpr, color='red',
         label=r'CNN (AUC≈%0.2f $\pm$%0.2f)' % (mean_auc, std_auc),
         lw=2, alpha=0.8)

### Using LSTM ##
print('Model#2: LSTM')
tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)
hists_LSTM = []

for train, test in kfold.split(X_encoded, Y):
  model = Sequential()
  model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_length))
  model.add(keras.layers.CuDNNLSTM(100))
  model.add(Dropout(0.2))
  model.add(Dense(1, activation='sigmoid'))
  
  adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])
  #print(model.summary())
  
  hist =model.fit(X_encoded[train], Y[train],batch_size=nBatch ,epochs=nEpoch, verbose=1,validation_split=0.30)
  pred = model.predict(X_encoded[test]).ravel()
  fpr, tpr, thresholds = roc_curve(Y[test], pred)
  tprs.append(interp(mean_fpr, fpr, tpr))
  tprs[-1][0] = 0.0
  roc_auc = auc(fpr, tpr)
  print(roc_auc)
  aucs.append(roc_auc)
  hists_LSTM.append(hist)
  

print("Avg AUC:", np.mean(aucs))

#Adding to ROC plot
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
plt.plot(mean_fpr, mean_tpr, color='blue',
         label=r'LSTM (AUC≈%0.2f $\pm$%0.2f)' % (mean_auc, std_auc),
         lw=2, alpha=.8)
plt.legend(loc="lower right")
end = time.time()
print("Training Time:", end - start, "seconds.")

#Plotting model loss in train and validation
fold = 0
hist_df = pd.DataFrame(hists_CNN[fold].history)

plt.plot(hist_df['loss'])
plt.plot(hist_df['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['loss', 'val_loss'], loc='upper right')
plt.show()